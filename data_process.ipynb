{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import sklearn.metrics.pairwise as skl\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import pickle\n",
    "import os\n",
    "import json\n",
    "import ast\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取 user-course.json 文件\n",
    "def read_user_course(file_path):\n",
    "    return pd.read_csv(file_path, sep=\"\\t\", header=None, names=[\"user_id\", \"course_id\"])\n",
    "\n",
    "# 读取 course-concept 文件\n",
    "def read_course_concept(file_path):\n",
    "    return pd.read_csv(file_path, sep=\"\\t\", header=None, names=[\"course_id\", \"concept\"])\n",
    "\n",
    "# 读取 course.json 文件\n",
    "def read_course_json(file_path):\n",
    "    data = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            course_data = json.loads(line)\n",
    "            data.append({'course_id': course_data['id'], 'name': course_data['name']})\n",
    "    return pd.DataFrame(data).rename(columns={'id': 'course_id', 'name': 'course_name'})\n",
    "\n",
    "\n",
    "# 读取 concept-field 文件\n",
    "def read_concept_field(file_path):\n",
    "    return pd.read_csv(file_path, sep=\"\\t\", header=None, names=[\"concept\", \"type\"])\n",
    "\n",
    "\n",
    "def parse_explanation(explanation):\n",
    "    # 使用正则表达式找出所有的 x:y 形式的模式\n",
    "    pattern = re.compile(r'(\\w+)：(.*?)\\s(?=\\w+：|$)')\n",
    "    matches = pattern.findall(explanation)\n",
    "    return dict(matches)\n",
    "\n",
    "def read_concept_json(file_path):\n",
    "    data = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            try:\n",
    "                concept_data = json.loads(line)\n",
    "                if 'explanation' in concept_data:\n",
    "                    explanation_dict = parse_explanation(concept_data['explanation'])\n",
    "                    if '学科' in explanation_dict:\n",
    "                        data.append({'concept': concept_data['id'], 'subtype': explanation_dict['学科']})\n",
    "            except json.JSONDecodeError:\n",
    "                continue  # 忽略无法解析的行\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "user_course_path = \"MOOCCube/relations/user-course.json\"\n",
    "course_concept_path = \"MOOCCube/relations/course-concept.json\"\n",
    "course_json_path = \"MOOCCube/entities/course.json\"\n",
    "concept_field_path = \"MOOCCube/relations/concept-field.json\"\n",
    "concept_json_path = \"MOOCCube/entities/concept.json\"\n",
    "\n",
    "\n",
    "# 读取数据\n",
    "user_course_df = read_user_course(user_course_path)\n",
    "course_concept_df = read_course_concept(course_concept_path)\n",
    "course_df = read_course_json(course_json_path)\n",
    "concept_field_df = read_concept_field(concept_field_path)\n",
    "concept_json_df = read_concept_json(concept_json_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 数据整合\n",
    "# 合并 user_course_df 和 course_concept_df\n",
    "merged_user_course_concept = pd.merge(user_course_df, course_concept_df, on=\"course_id\")\n",
    "\n",
    "# 对每个 user_id 和 course_id 组合聚合 concept\n",
    "grouped_user_course_concept = merged_user_course_concept.groupby(['user_id', 'course_id'])['concept'].apply(lambda x: list(set(x))).reset_index()\n",
    "\n",
    "\n",
    "\n",
    "# 合并 concept 和 concept-field\n",
    "merged_concept_type = pd.merge(course_concept_df, concept_field_df, on=\"concept\", how=\"left\")\n",
    "# 对每个 course_id 聚合 type\n",
    "grouped_course_type = merged_concept_type.groupby('course_id')['type'].apply(lambda x: list(set(x))).reset_index()\n",
    "\n",
    "\n",
    "# 合并 concept 和 concept.json\n",
    "merged_concept_subtype = pd.merge(course_concept_df, concept_json_df, on=\"concept\", how=\"left\")\n",
    "\n",
    "# 对每个 course_id 聚合 subtype\n",
    "grouped_course_subtype = merged_concept_subtype.groupby('course_id')['subtype'].apply(lambda x: list(set(x))).reset_index()\n",
    "\n",
    "# 合并所有数据\n",
    "final_merged = pd.merge(grouped_user_course_concept, grouped_course_type, on=\"course_id\")\n",
    "final_merged = pd.merge(final_merged, grouped_course_subtype, on=\"course_id\")\n",
    "final_merged = pd.merge(final_merged, course_df[['course_id', 'course_name']], on=\"course_id\", how=\"left\")\n",
    "\n",
    "# 确保 concept, user_id, course_id, name 列不为空\n",
    "final_merged.dropna(subset=['concept', 'user_id', 'course_id', 'course_name'], inplace=True)\n",
    "\n",
    "# 导出 CSV 文件\n",
    "final_csv_path = \"MOOCCube/data_full.csv\"\n",
    "final_merged.to_csv(final_csv_path, index=False)\n",
    "\n",
    "print(\"CSV 文件已生成:\", final_csv_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_df = pd.read_csv('MOOCCube/data_full.csv')\n",
    "\n",
    "# 初始化物品的知识图谱映射\n",
    "itemMap = {}\n",
    "count = 0\n",
    "csv_df['sub_type'] = csv_df['sub_type'].astype(str)\n",
    "\n",
    "# 为data.csv中的每个物品创建知识图谱\n",
    "for index, row in csv_df.iterrows():\n",
    "    item = row['course_index']\n",
    "    types = set(row['type'].split(' '))  # 从类型字段分割并创建集合\n",
    "    sub_types = set(row['sub_type'].replace(',', ' ').split())  # 替换逗号并分割创建集合\n",
    "    combined_set = types.union(sub_types)  # 合并两个集合\n",
    "    # 合并 type 和 sub_type 的值，存入 itemMap\n",
    "    itemMap[item] = combined_set\n",
    "\n",
    "\n",
    "# 保存每个物品的知识图谱到文件\n",
    "for item in itemMap:\n",
    "    # for feat in itemMap[item]:\n",
    "    count += len(itemMap[item])\n",
    "\n",
    "\n",
    "count = 0\n",
    "processed_course_ids = set()  # 用于跟踪已处理的课程ID\n",
    "\n",
    "for index, row in csv_df.iterrows():\n",
    "    item = row['course_id']\n",
    "    \n",
    "    # 检查课程ID是否已经处理过，如果是则跳过\n",
    "    if item in processed_course_ids:\n",
    "        continue\n",
    "    \n",
    "    # 将课程ID添加到已处理的集合中\n",
    "    processed_course_ids.add(item)\n",
    "    \n",
    "    # 将字符串形式的列表转换为实际的列表\n",
    "    concepts = ast.literal_eval(row['type'])\n",
    "    count += len(concepts)\n",
    "\n",
    "\n",
    "# 导出 CSV 文件\n",
    "final_csv_path = \"MOOCCube/data_full.csv\"\n",
    "final_merged.to_csv(final_csv_path, index=False)\n",
    "\n",
    "print(\"CSV 文件已生成:\", final_csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_df = pd.read_csv('MOOCCube/data_full.csv')\n",
    "\n",
    "# 筛选出交互次数大于或等于5次的用户\n",
    "user_interactions = csv_df['user_id'].value_counts()\n",
    "users_with_5_or_more_interactions = user_interactions[user_interactions >= 5].index\n",
    "filtered_by_user_interactions = csv_df[csv_df['user_id'].isin(users_with_5_or_more_interactions)]\n",
    "\n",
    "# 筛选出被10个以上用户交互过的课程\n",
    "course_interactions = filtered_by_user_interactions['course_id'].value_counts()\n",
    "courses_with_10_or_more_users = course_interactions[course_interactions >= 10].index\n",
    "filtered_by_course_interactions = filtered_by_user_interactions[filtered_by_user_interactions['course_id'].isin(courses_with_10_or_more_users)]\n",
    "\n",
    "# 确保没有 user_id 或 course_id 为空的行\n",
    "cleaned_df = filtered_by_course_interactions.dropna(subset=['user_id', 'course_id'])\n",
    "# 使用 factorize 方法重新编码 user_id 和 course_id\n",
    "cleaned_df['user_id'] = pd.factorize(cleaned_df['user_id'])[0]\n",
    "cleaned_df['course_id'] = pd.factorize(cleaned_df['course_id'])[0]\n",
    "\n",
    "# 重置索引\n",
    "cleaned_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# 统计独特的用户和课程数量\n",
    "unique_users = cleaned_df['user_id'].nunique()\n",
    "unique_courses = cleaned_df['course_id'].nunique()\n",
    "\n",
    "# 打印统计结果\n",
    "print(f\"筛选后的用户数量: {unique_users}\")\n",
    "print(f\"筛选后的课程数量: {unique_courses}\")\n",
    "\n",
    "final_csv_path = \"data_KEAM.csv\"\n",
    "cleaned_df.to_csv(final_csv_path, index=False)\n",
    "\n",
    "print(\"CSV 文件已生成:\", final_csv_path)\n",
    "\n",
    "# 添加固定值为 1 的列\n",
    "df['interaction'] = 1\n",
    "\n",
    "# 分割数据集为训练集和测试集，比例为 9:1\n",
    "train_data, test_data = train_test_split(df, test_size=0.1, random_state=42)\n",
    "\n",
    "# 只选择 user_id, course_id, interaction 列\n",
    "train_data = train_data[['user_id', 'course_id', 'interaction']]\n",
    "test_data = test_data[['user_id', 'course_id', 'interaction']]\n",
    "\n",
    "# 将训练集保存为 CSV 文件，使用制表符分隔，不包含表头\n",
    "train_data.to_csv(\"train_data.tsv\", sep='\\t', index=False, header=False)\n",
    "\n",
    "# 将测试集保存为 CSV 文件，使用制表符分隔，不包含表头\n",
    "test_data.to_csv(\"test_data.tsv\", sep='\\t', index=False, header=False)\n",
    "\n",
    "# 去重：确保每个 course_id 和 course_name 组合是唯一的\n",
    "unique_courses = df[['course_id', 'course_name']].drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "# 保存为 .tsv 文件\n",
    "unique_courses.to_csv('course_map.tsv', sep='\\t', index=False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
